{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Trees Assignment**"
      ],
      "metadata": {
        "id": "cL8b4XGZ7wGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **THEORY**"
      ],
      "metadata": {
        "id": "GkFR70QTFobr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "- A Decision Tree is a type of algorithm used in machine learning to make decisions — like answering questions step by step until you reach a conclusion.\n",
        "- Think of it like a flowchart that asks yes/no questions (or checks some conditions) to divide data into groups until it can make a final decision or prediction.\n",
        "\n",
        "##### In the Context of Classification\n",
        "\n",
        "- In classification, a decision tree helps us figure out which category or class something belongs to.\n",
        "\n",
        "## 2. Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "- Gini Impurity measures the probability that a randomly chosen item would be incorrectly classified if it was labeled according to the distribution of labels in the group.\n",
        "- Entropy measures the amount of randomness or uncertainty. Higher entropy = more disorder (mixed classes).\n",
        "\n",
        "#####  How They Impact Splits in a Decision Tree\n",
        "\n",
        "- At each node, the tree checks each feature and finds the best split — the one - that gives the lowest impurity after the split.\n",
        "- It calculates:\n",
        "  - Gini or Entropy for the current node (before split)\n",
        "  - Weighted average of Gini/Entropy after splitting by a feature\n",
        "  - Chooses the split that gives the highest impurity reduction\n",
        "\n",
        "## 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "- Pruning: Cutting off branches of a decision tree that are not useful or too specific, to make the model simpler and more general (better on new data).\n",
        "\n",
        "| Feature            | **Pre-Pruning (Early Stopping)**             | **Post-Pruning (Reduced Error Pruning)**           |\n",
        "| ------------------ | -------------------------------------------- | -------------------------------------------------- |\n",
        "| **When?**        | During tree building (early stop)            | After full tree is built                           |\n",
        "| **How?**        | Stops splitting a node if conditions met     | Grows full tree, then removes unnecessary branches |\n",
        "| **Criteria**    | Max depth, min samples at node, min impurity | Validation accuracy, error reduction after pruning |\n",
        "| **Overfitting?** | Tries to avoid it while growing the tree     | Handles it after seeing the full tree              |\n",
        "| **Complexity**  | Faster, needs fewer resources                | Slower but more accurate                           |\n",
        "\n",
        "##### Pre-Pruning - Example\n",
        "- Suppose you're building a tree to predict loan approval. You set:\n",
        "  - Max depth = 3\n",
        "  - Min samples to split = 10\n",
        "- If a node has less than 10 samples or depth reaches 3, it stops splitting even if it could improve accuracy on training data.\n",
        "\n",
        "- Advantage:\n",
        "  - Faster training time, prevents tree from becoming too large.\n",
        "\n",
        "##### Post-Pruning - Example\n",
        "- You first allow the tree to grow fully (even if it overfits), then:\n",
        "- Go back and remove branches that do not improve accuracy on a validation set.\n",
        "\n",
        "- Advantage:\n",
        "  - Produces a tree that is more generalizable to new data (better real-world performance).\n",
        "\n",
        "## 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "- Information Gain (IG) is a measure used to decide which feature to split on at each step of building a Decision Tree.\n",
        "- It tells us how much “information” or “purity” we gain by splitting the data based on a particular feature.\n",
        "- Information Gain = How much the uncertainty (Entropy) is reduced after the split.\n",
        "\n",
        "-  Formula: Information Gain = Entropy(Parent) - Weighted Avg Entropy (Children)\n",
        "\n",
        "###### Why is it Important?\n",
        "- The higher the Information Gain, the better the feature is at separating the data.\n",
        "- So, the decision tree chooses the feature with the highest IG for the split.\n",
        "- This helps the tree make more accurate and meaningful splits.\n",
        "\n",
        "## 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "##### Real-World Applications of Decision Trees\n",
        "\n",
        "| Area          | Application Example                                     |\n",
        "| ------------- | ------------------------------------------------------- |\n",
        "|  Education  | Predicting student performance                          |\n",
        "|  Healthcare | Diagnosing diseases based on symptoms                   |\n",
        "|  Banking    | Approving loans or detecting fraud                      |\n",
        "|  Retail     | Customer purchase prediction, segmentation              |\n",
        "|  HR      | Predicting employee attrition (will someone leave?)     |\n",
        "|  Telecom    | Predicting customer churn (will a user cancel service?) |\n",
        "\n",
        "##### Advantages of Decision Trees\n",
        "\n",
        "| Advantage                | Explanation                                      |\n",
        "| ------------------------ | ------------------------------------------------ |\n",
        "|  Easy to understand     | Works like a flowchart; no math degree needed!   |\n",
        "|  No need to scale data  | Works with both numeric and categorical features |\n",
        "|  Fast training          | Especially for small to medium datasets          |\n",
        "|  Handles missing values | Can work even if some values are missing         |\n",
        "\n",
        "#####  Limitations of Decision Trees\n",
        "\n",
        "| Limitation                                 | Explanation                                                   |\n",
        "| ------------------------------------------ | ------------------------------------------------------------- |\n",
        "|  Overfitting                              | Tree may become too complex and memorize the training data    |\n",
        "|  Unstable                                 | Small data changes can result in a completely different tree  |\n",
        "|  Biased towards features with more levels | Features with more categories may be chosen unfairly          |\n",
        "|  Not always the most accurate             | Often less accurate than models like Random Forest or XGBoost |"
      ],
      "metadata": {
        "id": "EYt5BPZtGw4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PRACTICAL**"
      ],
      "metadata": {
        "id": "u15YsKH6Fvjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Info:\n",
        "- Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "- Boston Housing Dataset for regression tasks\n",
        "  - (sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "6aYSX28IAs9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model's accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "prHyYswfAd2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 6\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data            # Features\n",
        "y = iris.target          # Labels\n",
        "\n",
        "# Step 2: Split the data into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier with Gini Index\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# Step 6: Print Feature Importances\n",
        "feature_names = iris.feature_names\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {round(importance, 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUFowrw8BFTr",
        "outputId": "16fe3487-18cb-404b-d3b9-82afbdf88ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.0 %\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "S8JlAmIHBO5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 7\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3a: Train fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 3b: Train pruned Decision Tree (max_depth=3)\n",
        "clf_pruned = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Step 4: Print the comparison\n",
        "print(\"Model Accuracy Comparison:\\n\")\n",
        "print(f\"Fully-grown Tree Accuracy   : {round(accuracy_full * 100, 2)}%\")\n",
        "print(f\"Pruned Tree (max_depth=3)   : {round(accuracy_pruned * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EOACFQCBgkr",
        "outputId": "e82f4dea-3727-46a8-8736-e246d7ec3176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy Comparison:\n",
            "\n",
            "Fully-grown Tree Accuracy   : 100.0%\n",
            "Pruned Tree (max_depth=3)   : 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "AxGtk5zFBuI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 8\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing  # Use load_boston() if using older version\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the housing dataset (alternative to Boston Housing)\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate Mean Squared Error (MSE)\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
        "\n",
        "# Step 5: Print Feature Importances\n",
        "print(\"\\n Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {round(importance, 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU53WAPxB9UG",
        "outputId": "0828ad49-6b61-4ebe-ff5e-53de7a7b441c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            " Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.053\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree's max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "Hb4CiaqqCJ4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 9\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Set up the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 10]\n",
        "}\n",
        "\n",
        "# Step 4: Create the Decision Tree model and apply GridSearchCV\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Output best parameters and accuracy\n",
        "print(\"Best Parameters Found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(\"\\n Accuracy of the Best Model:\")\n",
        "print(f\"{round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uydRUXXHCU7I",
        "outputId": "9ef04c43-18b7-437b-ad02-e215e79fdcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found by GridSearchCV:\n",
            "{'max_depth': 4, 'min_samples_split': 2}\n",
            "\n",
            " Accuracy of the Best Model:\n",
            "100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "- And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "FMBT5rakCe5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#  Step 1: Sample mock dataset (for demo purposes)\n",
        "# In real use, replace this with your CSV or real data\n",
        "\n",
        "data = {\n",
        "    'age': [25, 35, 45, np.nan, 50],\n",
        "    'blood_pressure': [120, 130, np.nan, 110, 140],\n",
        "    'gender': ['Male', 'Female', np.nan, 'Male', 'Female'],\n",
        "    'smoking_status': ['Never', 'Former', 'Current', 'Never', np.nan],\n",
        "    'has_disease': [0, 1, 1, 0, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('has_disease', axis=1)\n",
        "y = df['has_disease']\n",
        "\n",
        "# Step 2: Automatically detect column types\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Step 4: Encode categorical features\n",
        "encoder = ColumnTransformer(\n",
        "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
        "    remainder='passthrough'  # keep numeric columns\n",
        ")\n",
        "\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Step 5: Train-test split and Decision Tree\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxZMBd6fExp7",
        "outputId": "339f71ef-9ac2-4bcd-b496-680a2d5ebf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business Value in a Real-World Healthcare Setting\n",
        "\n",
        "| Area                     | Business Impact                                                               |\n",
        "| ------------------------ | ----------------------------------------------------------------------------- |\n",
        "| Early Diagnosis        | Predicts disease at early stages, enabling faster treatment                   |\n",
        "| Risk Stratification    | Classifies patients into high-risk/low-risk groups for preventive care        |\n",
        "| Resource Allocation    | Hospitals can better allocate beds, staff, and tests based on predicted needs |\n",
        "| Cost Reduction         | Prevents unnecessary tests for low-risk patients, saving money                |\n",
        "| Personalized Care      | Helps doctors make data-driven decisions customized to patient profiles       |\n",
        "| Compliance & Reporting | Decision trees provide interpretable logic for audits and medical validation  |"
      ],
      "metadata": {
        "id": "bEoxaS7oD23k"
      }
    }
  ]
}